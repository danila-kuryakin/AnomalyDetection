{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorchvideo \n!pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:37:19.594005Z","iopub.execute_input":"2023-06-10T00:37:19.594495Z","iopub.status.idle":"2023-06-10T00:37:59.135807Z","shell.execute_reply.started":"2023-06-10T00:37:19.594447Z","shell.execute_reply":"2023-06-10T00:37:59.134283Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting av\n  Downloading av-10.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting parameterized\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nCollecting iopath\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (2.6.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.21.6)\nRequirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.1.8)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (6.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (4.64.1)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (2.2.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (9.4.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (4.4.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (2.7.0)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188715 sha256=db012698cadde7d8fb9c2de770a9a42950322ccb3f29e546a423151d2752b8f4\n  Stored in directory: /root/.cache/pip/wheels/71/3c/ab/bfe50bbf6cfeea284d6179c59b31c463d55c68b6a10728ba20\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61431 sha256=1f022fd6f6b8551f19821ebdb80478d63acd6dde7c61f01c67dd985f0ea9f013\n  Stored in directory: /root/.cache/pip/wheels/12/a2/36/21b9bde5f8deeeb6312efe88ddde26a51facbd2089f32917b3\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=cc5a4f3780e125f71178ca7ee09ae059fb495e1e5821fd10ac0139b5d0323aa2\n  Stored in directory: /root/.cache/pip/wheels/96/9a/78/61eeeec98da40f44085da9ba3fec952b4ab7224f5c5be75126\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: av, parameterized, iopath, fvcore, pytorchvideo\nSuccessfully installed av-10.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 pytorchvideo-0.1.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting mediapipe\n  Downloading mediapipe-0.9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mediapipe) (1.21.6)\nRequirement already satisfied: protobuf<4,>=3.11 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (3.20.3)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (22.2.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mediapipe) (3.5.3)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (from mediapipe) (4.5.4.60)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (23.1.21)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (9.4.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (23.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (4.38.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\nInstalling collected packages: mediapipe\nSuccessfully installed mediapipe-0.9.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport math\nimport cv2\nimport pickle\nimport random\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import transforms\nimport torch.nn.init as init\nfrom torchvision import models\n\nimport mediapipe as mp\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-10T00:37:59.140614Z","iopub.execute_input":"2023-06-10T00:37:59.141037Z","iopub.status.idle":"2023-06-10T00:38:01.914527Z","shell.execute_reply.started":"2023-06-10T00:37:59.140994Z","shell.execute_reply":"2023-06-10T00:38:01.913426Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"label_path='list_patition_label.txt'\n\nweight_path = '/kaggle/input/dan-def-weight/rafdb_epoch21_acc0.897_bacc0.8275.pth'\n\nworkers=2\nbatch_size=64\nw=7\nh=7\ndevice_name=0\nlam=5\nepochs=100","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:01.916092Z","iopub.execute_input":"2023-06-10T00:38:01.916784Z","iopub.status.idle":"2023-06-10T00:38:01.922778Z","shell.execute_reply.started":"2023-06-10T00:38:01.916719Z","shell.execute_reply":"2023-06-10T00:38:01.921696Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class DAN(nn.Module):\n    def __init__(self, num_class=7,num_head=4, pretrained=True):\n        super(DAN, self).__init__()\n        \n        resnet = models.resnet18(pretrained)\n        \n        if pretrained:\n            checkpoint = torch.load('/kaggle/input/dan-def-weight/resnet18_msceleb.pth')\n            resnet.load_state_dict(checkpoint['state_dict'],strict=True)\n\n        self.features = nn.Sequential(*list(resnet.children())[:-2])\n        self.num_head = num_head\n        for i in range(num_head):\n            setattr(self,\"cat_head%d\" %i, CrossAttentionHead())\n        self.sig = nn.Sigmoid()\n        self.fc = nn.Linear(512, num_class)\n        self.bn = nn.BatchNorm1d(num_class)\n\n\n    def forward(self, x):\n        x = self.features(x)\n        heads = []\n        for i in range(self.num_head):\n            heads.append(getattr(self,\"cat_head%d\" %i)(x))\n        \n        heads = torch.stack(heads).permute([1,0,2])\n        if heads.size(1)>1:\n            heads = F.log_softmax(heads,dim=1)\n            \n        out = self.fc(heads.sum(dim=1))\n        out = self.bn(out)\n   \n        return out, x, heads\n\nclass CrossAttentionHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sa = SpatialAttention()\n        self.ca = ChannelAttention()\n        self.init_weights()\n\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n    def forward(self, x):\n        sa = self.sa(x)\n        ca = self.ca(sa)\n\n        return ca\n\n\nclass SpatialAttention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1x1 = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=1),\n            nn.BatchNorm2d(256),\n        )\n        self.conv_3x3 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3,padding=1),\n            nn.BatchNorm2d(512),\n        )\n        self.conv_1x3 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=(1,3),padding=(0,1)),\n            nn.BatchNorm2d(512),\n        )\n        self.conv_3x1 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=(3,1),padding=(1,0)),\n            nn.BatchNorm2d(512),\n        )\n        self.relu = nn.ReLU()\n\n\n    def forward(self, x):\n        y = self.conv1x1(x)\n        y = self.relu(self.conv_3x3(y) + self.conv_1x3(y) + self.conv_3x1(y))\n        y = y.sum(dim=1,keepdim=True) \n        out = x*y\n        \n        return out \n\nclass ChannelAttention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.attention = nn.Sequential(\n            nn.Linear(512, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(inplace=True),\n            nn.Linear(32, 512),\n            nn.Sigmoid()    \n        )\n\n\n    def forward(self, sa):\n        sa = self.gap(sa)\n        sa = sa.view(sa.size(0),-1)\n        y = self.attention(sa)\n        out = sa * y\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:01.926157Z","iopub.execute_input":"2023-06-10T00:38:01.926834Z","iopub.status.idle":"2023-06-10T00:38:01.956492Z","shell.execute_reply.started":"2023-06-10T00:38:01.926783Z","shell.execute_reply":"2023-06-10T00:38:01.955361Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def test(model, test_loader, device):\n    with torch.no_grad():\n        model.eval()\n\n        running_loss = 0.0\n        iter_cnt = 0\n        correct_sum = 0\n        data_num = 0\n\n        for batch_i, (imgs1, labels) in enumerate(test_loader):\n            \n            if batch_i%5 == 0:\n                print('Batch: {}/{}'.format(batch_i, len(test_loader)))\n            imgs1 = imgs1.to(device)\n            labels = labels.to(device)\n\n#             outputs, _ = model(imgs1)\n            outputs,feat,heads = model(imgs1)\n            \n            loss = nn.CrossEntropyLoss()(outputs, labels)\n\n            iter_cnt += 1\n            _, predicts = torch.max(outputs, 1)\n\n            correct_num = torch.eq(predicts, labels).sum()\n            correct_sum += correct_num\n\n            running_loss += loss\n            data_num += outputs.size(0)\n\n        running_loss = running_loss / iter_cnt\n        test_acc = correct_sum.float() / float(data_num)\n    return test_acc, running_loss","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:01.958249Z","iopub.execute_input":"2023-06-10T00:38:01.958664Z","iopub.status.idle":"2023-06-10T00:38:01.971996Z","shell.execute_reply.started":"2023-06-10T00:38:01.958622Z","shell.execute_reply":"2023-06-10T00:38:01.970956Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class ImageData(data.Dataset):\n    def __init__(self, df, data_dir, transform=None):\n        super().__init__()\n        self.df = df\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name = f'{self.data_dir}/{self.df.iloc[index][\"name\"]}'\n        label = self.df.iloc[index][\"label\"]-1\n        \n        image = cv2.imread(img_name)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        \n        if self.transform is not None:\n            image = self.transform(image)\n        \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:01.973915Z","iopub.execute_input":"2023-06-10T00:38:01.974264Z","iopub.status.idle":"2023-06-10T00:38:01.986853Z","shell.execute_reply.started":"2023-06-10T00:38:01.974233Z","shell.execute_reply":"2023-06-10T00:38:01.985700Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"labels = {'name': ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger', 'Neutral'],\n         'color': ['blue', 'orange', 'green', 'red', 'darkviolet', 'brown', 'pink']}","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:01.988611Z","iopub.execute_input":"2023-06-10T00:38:01.989235Z","iopub.status.idle":"2023-06-10T00:38:01.997439Z","shell.execute_reply.started":"2023-06-10T00:38:01.989171Z","shell.execute_reply":"2023-06-10T00:38:01.996329Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def plot_probability(data, title,  figsize, save_dir):\n    file_name = os.path.join(save_dir, title + '.png')    \n    fig, ax = plt.subplots(figsize=figsize)\n    \n    \n#     print([x for x in range(0, len(data))])\n    ylabel = [x for x in range(0, len(data))]\n#     print(data)\n    ax.plot(ylabel, data)\n    ax.set_title(title)\n    \n    for index, line in enumerate(plt.gca().get_lines()):\n        line.set_color(label['color'][index])\n    \n    ax.legend(label['name'],\n                  loc='upper left')\n    \n    fig.savefig(file_name)\n    plt.close(fig)\n        \ndef plot_classes(data, title,  figsize):\n#     index = 0\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(data)\n    ax.set_title(title)\n    ax.set_ylim([-1, 7])\n    \ndef save_data(data, title, save_dir):\n    file_name = os.path.join(save_dir, title + '.txt')\n    with open(file_name, 'w') as open_file:\n        open_file.write('index probabilities\\n')\n        for index, row in enumerate(data):\n            open_file.write(str(index) + ' '+ ' '.join(str(r) for r in row) + '\\n')\n        open_file.close()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:01.999403Z","iopub.execute_input":"2023-06-10T00:38:01.999839Z","iopub.status.idle":"2023-06-10T00:38:02.012463Z","shell.execute_reply.started":"2023-06-10T00:38:01.999801Z","shell.execute_reply":"2023-06-10T00:38:02.011271Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def detect(model, video_path, mp_face_detection, save_dir=''):\n    with torch.no_grad():\n        softmax = nn.Softmax(dim=1)\n        model.eval()\n        probability = []\n        class_list = []\n\n        eval_transforms = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])])\n\n        cap = cv2.VideoCapture(video_path)\n        while (cap.isOpened()):\n            ret, frame = cap.read()\n            if ret == True:\n                coord = face_detection(frame, mp_face_detection)\n                if coord:\n                    (x1, y1, x2, y2) = coord\n                    face = frame[y1:y2, x1:x2, :]\n                else:\n                    face = frame\n                val2_loader = torch.utils.data.DataLoader(face, batch_size=1,\n                                                  shuffle=False,\n                                                  num_workers=workers,\n                                                  pin_memory=True)\n                face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n                face = cv2.resize(face, (224, 224))\n    #             plt.imshow(face)\n                face = eval_transforms(face)\n                face = face.reshape((1, 3, 224, 224))\n                face = face.to(device)\n\n                outputs = model(face)\n                sm = softmax(outputs[0])\n                am = torch.argmax(outputs[0])\n                probability.append(sm.detach().cpu().numpy()[0].tolist())\n                class_list.append(am.detach().cpu().tolist())\n            else:\n                break\n        probability = np.array(probability)\n        class_ar = np.array(class_list)\n\n        video_name = video_path.split('/')[-1]\n        video_name = video_name.split('.')[0]\n\n#         plot_probability(probability, video_name, (8, 5), save_dir)\n#         save_data(probability, video_name, save_dir)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:02.014545Z","iopub.execute_input":"2023-06-10T00:38:02.014997Z","iopub.status.idle":"2023-06-10T00:38:02.033160Z","shell.execute_reply.started":"2023-06-10T00:38:02.014959Z","shell.execute_reply":"2023-06-10T00:38:02.031960Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def face_detection(image, mp_face_detection):\n    h, w, _ = image.shape\n    with mp_face_detection.FaceDetection(\n        model_selection=1, min_detection_confidence=0.5) as face_detection:\n\n        results = face_detection.process(\n            cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        detections = results.detections\n        if detections:\n            detection = detections[0]\n            x1 = int(w * detection.location_data.relative_bounding_box.xmin)\n            y1 = int(h * detection.location_data.relative_bounding_box.ymin)\n\n            if x1 < 0:\n                x1 = 0\n            if y1 < 0:\n                y1 = 0\n\n            width = w * detection.location_data.relative_bounding_box.width\n            height = h * detection.location_data.relative_bounding_box.height\n            x2 = int(w * detection.location_data.relative_bounding_box.xmin + width)\n            y2 = int(h * detection.location_data.relative_bounding_box.ymin + height)\n            if x2 > w:\n                x2 = w\n            if y2 > h:\n                y2 = h\n            return [x1, y1, x2, y2]\n    return None","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:02.039339Z","iopub.execute_input":"2023-06-10T00:38:02.039705Z","iopub.status.idle":"2023-06-10T00:38:02.052343Z","shell.execute_reply.started":"2023-06-10T00:38:02.039664Z","shell.execute_reply":"2023-06-10T00:38:02.051274Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"mp_face_detection = mp.solutions.face_detection\n\nvideo_path = '/kaggle/input/ckp-video/Train/S503_001.avi'\n\nVAL_CSV = pd.read_csv(\n    '/kaggle/input/ckp-video/val.txt',\n    sep=' ',\n)\n\nVAL_DIR = '/kaggle/input/ckp-video/Val'\n\nTRAIN_CSV = pd.read_csv(\n    '/kaggle/input/ckp-video/train.txt',\n    sep=' ',\n)\n\nTRAIN_DIR = '/kaggle/input/ckp-video/Train'\n\nmodel = DAN(num_head=4)\nmodel.load_state_dict(torch.load(weight_path)['model_state_dict'])\ndevice = torch.device(device_name)\nmodel.to(device)\n\n# save_dir = 'val'\n# if not os.path.exists(save_dir):\n#     os.makedirs(save_dir)\n\n# for name in VAL_CSV['name']:\n#     video_path = os.path.join(VAL_DIR, name)\n#     detect(model, video_path, mp_face_detection, save_dir)\n    \n    \n# save_dir = 'train'\n# if not os.path.exists(save_dir):\n#     os.makedirs(save_dir)\n\n# for name in TRAIN_CSV['name']:\n#     video_path = os.path.join(TRAIN_DIR, name)\n#     detect(model, video_path, mp_face_detection, save_dir)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:02.055667Z","iopub.execute_input":"2023-06-10T00:38:02.056027Z","iopub.status.idle":"2023-06-10T00:38:08.634981Z","shell.execute_reply.started":"2023-06-10T00:38:02.055998Z","shell.execute_reply":"2023-06-10T00:38:08.633760Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:136: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  f\"Using {sequence_to_str(tuple(keyword_only_kwargs.keys()), separate_last='and ')} as positional \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/44.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959418e6f23e4105a08b1fd50434d78d"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DAN(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (cat_head0): CrossAttentionHead(\n    (sa): SpatialAttention(\n      (conv1x1): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_3x3): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_1x3): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_3x1): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (relu): ReLU()\n    )\n    (ca): ChannelAttention(\n      (gap): AdaptiveAvgPool2d(output_size=1)\n      (attention): Sequential(\n        (0): Linear(in_features=512, out_features=32, bias=True)\n        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Linear(in_features=32, out_features=512, bias=True)\n        (4): Sigmoid()\n      )\n    )\n  )\n  (cat_head1): CrossAttentionHead(\n    (sa): SpatialAttention(\n      (conv1x1): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_3x3): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_1x3): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_3x1): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (relu): ReLU()\n    )\n    (ca): ChannelAttention(\n      (gap): AdaptiveAvgPool2d(output_size=1)\n      (attention): Sequential(\n        (0): Linear(in_features=512, out_features=32, bias=True)\n        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Linear(in_features=32, out_features=512, bias=True)\n        (4): Sigmoid()\n      )\n    )\n  )\n  (cat_head2): CrossAttentionHead(\n    (sa): SpatialAttention(\n      (conv1x1): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_3x3): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_1x3): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_3x1): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (relu): ReLU()\n    )\n    (ca): ChannelAttention(\n      (gap): AdaptiveAvgPool2d(output_size=1)\n      (attention): Sequential(\n        (0): Linear(in_features=512, out_features=32, bias=True)\n        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Linear(in_features=32, out_features=512, bias=True)\n        (4): Sigmoid()\n      )\n    )\n  )\n  (cat_head3): CrossAttentionHead(\n    (sa): SpatialAttention(\n      (conv1x1): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_3x3): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_1x3): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv_3x1): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (relu): ReLU()\n    )\n    (ca): ChannelAttention(\n      (gap): AdaptiveAvgPool2d(output_size=1)\n      (attention): Sequential(\n        (0): Linear(in_features=512, out_features=32, bias=True)\n        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Linear(in_features=32, out_features=512, bias=True)\n        (4): Sigmoid()\n      )\n    )\n  )\n  (sig): Sigmoid()\n  (fc): Linear(in_features=512, out_features=7, bias=True)\n  (bn): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# import shutil\n# from IPython.display import FileLink\n\n# shutil.make_archive('/kaggle/working/val', 'zip', '/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:08.636620Z","iopub.execute_input":"2023-06-10T00:38:08.637220Z","iopub.status.idle":"2023-06-10T00:38:08.643186Z","shell.execute_reply.started":"2023-06-10T00:38:08.637178Z","shell.execute_reply":"2023-06-10T00:38:08.641960Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# shutil.make_archive('/kaggle/working/val', 'zip', '/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:08.644468Z","iopub.execute_input":"2023-06-10T00:38:08.645187Z","iopub.status.idle":"2023-06-10T00:38:08.653644Z","shell.execute_reply.started":"2023-06-10T00:38:08.645146Z","shell.execute_reply":"2023-06-10T00:38:08.652559Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def frame_count(video_path):\n    with torch.no_grad():\n        frame_counter = 0\n        cap = cv2.VideoCapture(video_path)\n        while (cap.isOpened()):\n            ret, frame = cap.read()\n            if ret == True:\n                frame_counter += 1\n            else:\n                break\n        return frame_counter","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:08.656207Z","iopub.execute_input":"2023-06-10T00:38:08.656547Z","iopub.status.idle":"2023-06-10T00:38:08.666908Z","shell.execute_reply.started":"2023-06-10T00:38:08.656515Z","shell.execute_reply":"2023-06-10T00:38:08.665814Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from statistics import mean\n\nlist_frames = []\nfor name in VAL_CSV['name']:\n    video_path = os.path.join(VAL_DIR, name)\n    list_frames.append(frame_count(video_path))\n#     \nfor name in TRAIN_CSV['name']:\n    video_path = os.path.join(TRAIN_DIR, name)\n    list_frames.append(frame_count(video_path))\n    \n# print(list_frames)\n    \nprint('min', min(list_frames))\nprint('max', max(list_frames))\nprint('average', int(mean(list_frames)))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:08.668329Z","iopub.execute_input":"2023-06-10T00:38:08.668801Z","iopub.status.idle":"2023-06-10T00:38:11.811053Z","shell.execute_reply.started":"2023-06-10T00:38:08.668759Z","shell.execute_reply":"2023-06-10T00:38:11.809885Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"min 6\nmax 71\naverage 17\n","output_type":"stream"}]},{"cell_type":"code","source":"def detect2(model, video_path, mp_face_detection, save_dir=''):\n    with torch.no_grad():\n        softmax = nn.Softmax(dim=1)\n        model.eval()\n        probability_list = []\n        predictions_list = []\n        counter = 0\n\n        eval_transforms = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])])\n\n        cap = cv2.VideoCapture(video_path)\n        while (cap.isOpened()):\n            ret, frame = cap.read()\n            if ret == True:\n                counter += 1\n                coord = face_detection(frame, mp_face_detection)\n                if coord:\n                    (x1, y1, x2, y2) = coord\n                    face = frame[y1:y2, x1:x2, :]\n                else:\n                    face = frame\n                val2_loader = torch.utils.data.DataLoader(face, batch_size=1,\n                                                  shuffle=False,\n                                                  num_workers=workers,\n                                                  pin_memory=True)\n                face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n                face = cv2.resize(face, (224, 224))\n                face = eval_transforms(face)\n                face = face.reshape((1, 3, 224, 224))\n                face = face.to(device)\n\n                outputs = model(face)\n                sm = softmax(outputs[0])\n                am = torch.argmax(outputs[0])\n                probability_list.append(sm.detach().cpu().numpy()[0].tolist())\n                predictions_list.append(am.detach().cpu().tolist())\n            else:\n                break\n        probability_arr = np.array(probability_list)\n        predictions_arr = np.array(predictions_list)\n        return predictions_arr, probability_arr","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:11.812628Z","iopub.execute_input":"2023-06-10T00:38:11.813283Z","iopub.status.idle":"2023-06-10T00:38:11.829844Z","shell.execute_reply.started":"2023-06-10T00:38:11.813229Z","shell.execute_reply":"2023-06-10T00:38:11.828736Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def to_label(key):\n# CKp 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise\n# RAF 0: Surprise 1: Fear 2: Disgust 3: Happiness 4: Sadness 5: Anger 6: Neutral\n    labeling = {0:6, 1:5, 2:-1, 3:2, 4:1, 5:3, 6:4, 7:0}\n    return labeling[key]\n\ndef list_to_class(arr):\n    numer_of_classes = 7\n    counter = np.zeros(numer_of_classes)\n    for element in arr:\n        counter[element] += 1\n#     print(counter)\n    return np.argmax(counter)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:11.832836Z","iopub.execute_input":"2023-06-10T00:38:11.833275Z","iopub.status.idle":"2023-06-10T00:38:12.438040Z","shell.execute_reply.started":"2023-06-10T00:38:11.833233Z","shell.execute_reply":"2023-06-10T00:38:12.436811Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# equal_train = 0\n# not_equal_train = 0\n# equal_val = 0\n# not_equal_val = 0\n\n# for name, label in zip(TRAIN_CSV['name'], TRAIN_CSV['label']):\n#     new_label = to_label(label)\n#     if new_label == -1:\n#         continue\n#     video_path = os.path.join(TRAIN_DIR, name)\n#     predictions, probability = detect2(model, video_path, mp_face_detection, save_dir)\n    \n#     if to_label(label) == list_to_class(predictions):\n#         equal_train += 1\n#     else:\n#         not_equal_train += 1\n    \n# print('TRAIN: equal', equal_train, 'not equal', not_equal_train, 'accuracu', equal_train/(equal_train+not_equal_train))\n\n# for name, label in zip(VAL_CSV['name'], VAL_CSV['label']):\n#     new_label = to_label(label)\n#     if new_label == -1:\n#         continue\n#     video_path = os.path.join(VAL_DIR, name)\n#     predictions, probability = detect2(model, video_path, mp_face_detection, save_dir)\n    \n#     if to_label(label) == list_to_class(predictions):\n#         equal_val += 1\n#     else:\n#         not_equal_val += 1\n    \n# print('VALIDATION: equal', equal_val, 'not equal', not_equal_val, 'accuracu', equal_val/(equal_val+not_equal_val))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:12.440013Z","iopub.execute_input":"2023-06-10T00:38:12.440553Z","iopub.status.idle":"2023-06-10T00:38:12.452805Z","shell.execute_reply.started":"2023-06-10T00:38:12.440510Z","shell.execute_reply":"2023-06-10T00:38:12.451669Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def quantitative_analysis(predictions):\n    counter = np.zeros(NUMBER_CLASSES)\n    for element in predictions:\n        counter[element] += 1\n    if sum(counter==np.amax(counter)) > 1:\n        return 1\n    return 0\n\ndef outlier_analysis(predictions):\n    single_counter = 0\n    double_counter = 0\n    triple_counter = 0\n    for index in range(0, len(predictions)-2):\n        if predictions[index] == predictions[index+2] and predictions[index] != predictions[index+1]:\n            single_counter += 1\n        if index < (len(predictions)-3) and predictions[index] == predictions[index+3] and predictions[index] != predictions[index+1] and predictions[index] != predictions[index+2]:\n            double_counter += 1\n        if index < (len(predictions)-4) and predictions[index] == predictions[index+4] and predictions[index] != predictions[index+1] and predictions[index] != predictions[index+2] and predictions[index] != predictions[index+3]:\n            triple_counter += 1\n    return single_counter, double_counter, triple_counter\n\ndef chatter_analysis(predictions):\n    series_list = []\n    counter = 2\n    for index in range(0, len(predictions)-2):\n        if predictions[index] != predictions[index+2] and predictions[index] != predictions[index+1] and predictions[index+1] != predictions[index+2]:\n            counter += 1\n        else:\n            if counter > 3:\n                series_list.append(counter+2)\n            counter = 2\n    return series_list\n    \n\ndef predictions_analysis(predictions):\n    outliers = np.zeros(3)\n    \n    frames_uncertainty = quantitative_analysis(predictions)\n        \n    outliers[0], outliers[1], outliers[2] = outlier_analysis(predictions)\n    chatter_series = chatter_analysis(predictions)\n        \n    return frames_uncertainty, outliers, chatter_series\n\ndef average_plot(probabilities, average):\n    \n    return 0\n\ndef average(probabilities, window_size):\n    out = []\n    if len(probabilities) > window_size:\n        out.append([sum(probabilities[x-window_size:x])/window_size for x in range(window_size, len(probabilities))])\n    \n    print(out)\n    return out\n\n\ndef probability_analysis(probabilities):\n    \n    error_arr = np.zeros(1)\n    series_list = []\n    series = 0\n    for  i, probability_frames in enumerate(probabilities):\n        counter = 0\n        for probability in probability_frames:\n            if probability > MAX_PORABILITIES:\n                counter += 1\n        if counter > 1:\n            error_arr[0] += 1\n            series += 1\n        else:\n            if series > 1:\n                series_list.append(series)\n            series = 0\n\n    return error_arr, series_list\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:12.454554Z","iopub.execute_input":"2023-06-10T00:38:12.455175Z","iopub.status.idle":"2023-06-10T00:38:12.477949Z","shell.execute_reply.started":"2023-06-10T00:38:12.455132Z","shell.execute_reply":"2023-06-10T00:38:12.476724Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"NUMBER_CLASSES = 7\ngol_frames = 71\n\nequal_val = 0\nnot_equal_val = 0\nvalidation_result = []\n\nval_probabilities = []\nval_predictions = []\nval_label = []\n\nval_video_predictions = []\nval_covolution_matrix = np.zeros((7, 7))\n\ntrue_positive = 0\ntrue_negative = 0\nfalse_positive = 0\nfalse_negative = 0\n\n\nfor name, label in zip(VAL_CSV['name'], VAL_CSV['label']):\n    new_label = to_label(label)\n    if new_label == -1:\n        continue\n    video_path = os.path.join(VAL_DIR, name)\n    predictions, probabilities = detect2(model, video_path, mp_face_detection)\n    adepted_probabilities = np.zeros((gol_frames, 7))\n    adepted_probabilities[0: len(probabilities)] = probabilities\n    \n    adepted_predictions = np.zeros((gol_frames))\n    adepted_predictions[0: len(predictions)] = predictions\n    for index in range(len(predictions)+1, gol_frames):\n        adepted_probabilities[index] = probabilities[-1:]\n        adepted_predictions[index] = predictions[-1:]\n    \n    \n    frames_uncertainty, outliers, chatter_series = predictions_analysis(predictions)\n    \n    validation_result.append([name, frames_uncertainty, outliers, chatter_series])\n    val_probabilities.append(adepted_probabilities)\n    val_predictions.append(adepted_predictions)\n    val_label.append(label)\n    \n    val_video_predictions.append(list_to_class(predictions))\n    if new_label == list_to_class(predictions):\n        true_positive += 1\n        if frames_uncertainty or len(chatter_series) or sum(outliers):\n            true_negative += 1\n    else:\n        false_positive += 1\n        if (frames_uncertainty>0) or len(chatter_series) or sum(outliers):\n            false_negative += 1\n        \n    val_covolution_matrix[list_to_class(predictions), new_label] += 1\n      \nprecision = true_positive/(true_positive + false_positive)\nrecall = true_positive/(true_positive + false_negative)\nf_score = (2 * recall * precision)/(recall + precision)\n\naccuracy = (true_positive + true_negative)/(true_positive + true_negative + false_positive + false_negative)\n\npd.DataFrame([true_positive, true_negative, false_positive, false_negative, precision, recall, f_score, accuracy], \n             index=['true positive', 'true negative', 'false positive', 'false negative', 'precision', 'recall', 'F score', 'accuracy'])\n\n# print('VALIDATION: equal', true_positive, \n#       ',not equal', false_positive, \n#       'precision', precision, \n#       'recall', recall,\n#       'F_score', f_score)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:38:12.480127Z","iopub.execute_input":"2023-06-10T00:38:12.480939Z","iopub.status.idle":"2023-06-10T00:39:06.848826Z","shell.execute_reply.started":"2023-06-10T00:38:12.480897Z","shell.execute_reply":"2023-06-10T00:39:06.847820Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                        0\ntrue positive   38.000000\ntrue negative    4.000000\nfalse positive  18.000000\nfalse negative   3.000000\nprecision        0.678571\nrecall           0.926829\nF score          0.783505\naccuracy         0.666667","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>true positive</th>\n      <td>38.000000</td>\n    </tr>\n    <tr>\n      <th>true negative</th>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>false positive</th>\n      <td>18.000000</td>\n    </tr>\n    <tr>\n      <th>false negative</th>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>precision</th>\n      <td>0.678571</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.926829</td>\n    </tr>\n    <tr>\n      <th>F score</th>\n      <td>0.783505</td>\n    </tr>\n    <tr>\n      <th>accuracy</th>\n      <td>0.666667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pd.DataFrame(val_covolution_matrix, columns=labels['name'], index=labels['name'])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:39:06.850461Z","iopub.execute_input":"2023-06-10T00:39:06.851020Z","iopub.status.idle":"2023-06-10T00:39:06.875219Z","shell.execute_reply.started":"2023-06-10T00:39:06.850978Z","shell.execute_reply":"2023-06-10T00:39:06.874049Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"           Surprise  Fear  Disgust  Happiness  Sadness  Anger  Neutral\nSurprise       11.0   0.0      0.0        0.0      0.0    0.0      0.0\nFear            0.0   1.0      0.0        0.0      0.0    0.0      0.0\nDisgust         0.0   0.0      4.0        0.0      0.0    1.0      0.0\nHappiness       1.0   0.0      1.0       18.0      0.0    0.0      0.0\nSadness         0.0   1.0      0.0        0.0      4.0    4.0      0.0\nAnger           0.0   0.0      0.0        0.0      0.0    0.0      0.0\nNeutral         3.0   1.0      3.0        0.0      0.0    3.0      0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Surprise</th>\n      <th>Fear</th>\n      <th>Disgust</th>\n      <th>Happiness</th>\n      <th>Sadness</th>\n      <th>Anger</th>\n      <th>Neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Surprise</th>\n      <td>11.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Fear</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Disgust</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Happiness</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Sadness</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Anger</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Neutral</th>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# pd.DataFrame(validation_result, columns=['name', 'frames uncertainty', 'outliers[single double triple]', 'chatter series'])\n# pd.DataFrame(validation_result, columns=['Имя', 'Неопределенность в кадрак', 'Выброс[один два три]', 'Шум'])\n# вврппрпор","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:39:06.877345Z","iopub.execute_input":"2023-06-10T00:39:06.877708Z","iopub.status.idle":"2023-06-10T00:39:06.883837Z","shell.execute_reply.started":"2023-06-10T00:39:06.877677Z","shell.execute_reply":"2023-06-10T00:39:06.882582Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"equal_train = 0\nnot_equal_train = 0\ntrain_result = []\n\ntrain_probabilities = []\ntrain_predictions = []\ntrain_label = []\n\nfor name, label in zip(TRAIN_CSV['name'], TRAIN_CSV['label']):\n    new_label = to_label(label)\n    if new_label == -1:\n        continue\n    video_path = os.path.join(TRAIN_DIR, name)\n    predictions, probabilities = detect2(model, video_path, mp_face_detection)\n    \n    adepted_predictions = np.zeros((gol_frames))\n    adepted_predictions[0: len(predictions)] = predictions\n    for index in range(len(probabilities)+1, gol_frames):\n        adepted_probabilities[index] = probabilities[-1:]\n        adepted_predictions[index] = predictions[-1:]\n    \n    frames_uncertainty, outliers, chatter_series = predictions_analysis(predictions)\n#     probability_error, series_uncertainty = probability_analysis(probabilities)\n    \n    train_result.append([name, frames_uncertainty, outliers, chatter_series])\n    train_probabilities.append(adepted_probabilities)\n    train_predictions.append(adepted_predictions)\n    train_label.append(label)\n    \n    if to_label(label) == list_to_class(predictions):\n        equal_train += 1\n    else:\n        not_equal_train += 1\n\n    if new_label == list_to_class(predictions):\n        true_positive += 1\n        if frames_uncertainty or len(chatter_series) or sum(outliers):\n            true_negative += 1\n    else:\n        false_positive += 1\n        if (frames_uncertainty>0) or len(chatter_series) or sum(outliers):\n            false_negative += 1\n        \n        \nprecision = true_positive/(true_positive + false_positive)\nrecall = true_positive/(true_positive + false_negative)\nf_score = (2 * recall * precision)/(recall + precision)\naccuracy = (true_positive + true_negative)/(true_positive + true_negative + false_positive + false_negative)\n\npd.DataFrame([true_positive, true_negative, false_positive, false_negative, precision, recall, f_score, accuracy], \n             index=['true positive', 'true negative', 'false positive', 'false negative', 'precision', 'recall', 'F score', 'accuracy'])\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:39:06.885584Z","iopub.execute_input":"2023-06-10T00:39:06.886080Z","iopub.status.idle":"2023-06-10T00:42:48.257119Z","shell.execute_reply.started":"2023-06-10T00:39:06.886048Z","shell.execute_reply":"2023-06-10T00:42:48.256018Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                         0\ntrue positive   194.000000\ntrue negative    32.000000\nfalse positive  115.000000\nfalse negative   33.000000\nprecision         0.627832\nrecall            0.854626\nF score           0.723881\naccuracy          0.604278","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>true positive</th>\n      <td>194.000000</td>\n    </tr>\n    <tr>\n      <th>true negative</th>\n      <td>32.000000</td>\n    </tr>\n    <tr>\n      <th>false positive</th>\n      <td>115.000000</td>\n    </tr>\n    <tr>\n      <th>false negative</th>\n      <td>33.000000</td>\n    </tr>\n    <tr>\n      <th>precision</th>\n      <td>0.627832</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.854626</td>\n    </tr>\n    <tr>\n      <th>F score</th>\n      <td>0.723881</td>\n    </tr>\n    <tr>\n      <th>accuracy</th>\n      <td>0.604278</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('TRAIN: equal', equal_train, 'not equal', not_equal_train, 'accuracu', equal_train/(equal_train+not_equal_train))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:42:48.258849Z","iopub.execute_input":"2023-06-10T00:42:48.259570Z","iopub.status.idle":"2023-06-10T00:42:48.266290Z","shell.execute_reply.started":"2023-06-10T00:42:48.259515Z","shell.execute_reply":"2023-06-10T00:42:48.265033Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"TRAIN: equal 156 not equal 97 accuracu 0.616600790513834\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame(train_result, columns=['name', 'frames uncertainty', 'outliers[single double triple]', 'chatter series'])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:42:48.268034Z","iopub.execute_input":"2023-06-10T00:42:48.268731Z","iopub.status.idle":"2023-06-10T00:42:48.308138Z","shell.execute_reply.started":"2023-06-10T00:42:48.268691Z","shell.execute_reply":"2023-06-10T00:42:48.307163Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"             name  frames uncertainty outliers[single double triple]  \\\n0    S005_001.avi                   0                [0.0, 0.0, 0.0]   \n1    S010_002.avi                   0                [0.0, 0.0, 0.0]   \n2    S010_004.avi                   0                [0.0, 0.0, 0.0]   \n3    S010_006.avi                   0                [0.0, 0.0, 0.0]   \n4    S011_001.avi                   0                [1.0, 0.0, 1.0]   \n..            ...                 ...                            ...   \n248  S504_006.avi                   0                [3.0, 0.0, 0.0]   \n249  S505_006.avi                   0                [0.0, 0.0, 0.0]   \n250  S506_004.avi                   0                [0.0, 0.0, 0.0]   \n251  S999_001.avi                   0                [1.0, 0.0, 0.0]   \n252  S999_003.avi                   1                [5.0, 2.0, 2.0]   \n\n    chatter series  \n0               []  \n1               []  \n2               []  \n3               []  \n4               []  \n..             ...  \n248             []  \n249             []  \n250             []  \n251             []  \n252             []  \n\n[253 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>frames uncertainty</th>\n      <th>outliers[single double triple]</th>\n      <th>chatter series</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>S005_001.avi</td>\n      <td>0</td>\n      <td>[0.0, 0.0, 0.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>S010_002.avi</td>\n      <td>0</td>\n      <td>[0.0, 0.0, 0.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>S010_004.avi</td>\n      <td>0</td>\n      <td>[0.0, 0.0, 0.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>S010_006.avi</td>\n      <td>0</td>\n      <td>[0.0, 0.0, 0.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>S011_001.avi</td>\n      <td>0</td>\n      <td>[1.0, 0.0, 1.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>S504_006.avi</td>\n      <td>0</td>\n      <td>[3.0, 0.0, 0.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>S505_006.avi</td>\n      <td>0</td>\n      <td>[0.0, 0.0, 0.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>250</th>\n      <td>S506_004.avi</td>\n      <td>0</td>\n      <td>[0.0, 0.0, 0.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>251</th>\n      <td>S999_001.avi</td>\n      <td>0</td>\n      <td>[1.0, 0.0, 0.0]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>252</th>\n      <td>S999_003.avi</td>\n      <td>1</td>\n      <td>[5.0, 2.0, 2.0]</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>253 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# class AirModel(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.lstm = nn.LSTM(input_size=7, hidden_size=gol_frames, num_layers=1, batch_first=True)\n#         self.linear = nn.Linear(gol_frames, 1)\n#     def forward(self, x):\n#         l1, _ = self.lstm(x)\n#         out = self.linear(l1)\n#         return out\n    \n# model = AirModel()\n# optimizer = torch.optim.Adam(model.parameters())\n# loss_fn = nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:42:48.310494Z","iopub.execute_input":"2023-06-10T00:42:48.311196Z","iopub.status.idle":"2023-06-10T00:42:48.317839Z","shell.execute_reply.started":"2023-06-10T00:42:48.311157Z","shell.execute_reply":"2023-06-10T00:42:48.316607Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# # print(np.array(train_probabilities).shape)\n# # print(np.array(train_label_list).shape)\n\n# X_train = torch.tensor(train_probabilities)\n# X_train = X_train.float()\n# # print(X_train)\n# train_label_list = np.zeros((len(train_label), 7))\n# for index, label in enumerate(train_label):\n#     train_label_list[index][label-1] = 1\n# y_train = torch.tensor(train_label_list)\n# y_train = y_train.float()\n\n# X_val = torch.tensor(val_probabilities)\n# X_val = X_val.float()\n# val_label_list = np.zeros((len(val_label), 7))\n# for index, label in enumerate(val_label):\n#     val_label_list[index][label-1] = 1\n# y_val = torch.tensor(val_label_list)\n# y_val = y_val.float()\n\n# loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n \n# n_epochs = 2000\n# for epoch in range(n_epochs):\n#     model.train()\n#     for X_batch, y_batch in loader:\n        \n#         y_pred = model(X_batch)\n#         _, predicts = torch.max(y_pred, 1)\n#         print(predicts, y_batch)\n#         loss = loss_fn(predicts, y_batch)\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n#     # Validation\n#     if epoch % 100 != 0:\n#         continue\n#     model.eval()\n#     with torch.no_grad():\n#         y_train_pred = model(X_train)\n#         train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n        \n        \n#         y_val_pred = model(X_val)\n#         test_rmse = np.sqrt(loss_fn(y_pred, y_val))\n        \n        \n#     print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse), len(y_train_pred), len(y_val_pred))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:42:48.320722Z","iopub.execute_input":"2023-06-10T00:42:48.321068Z","iopub.status.idle":"2023-06-10T00:42:48.328784Z","shell.execute_reply.started":"2023-06-10T00:42:48.321039Z","shell.execute_reply":"2023-06-10T00:42:48.327981Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# # import torch.nn as nn\n\n# class RNN_LSTM_Base(nn.Module):\n#     def training_step(self, batch):\n#         samples, targets = batch\n#         outputs = self(samples.double())\n#         loss = nn.functional.mse_loss(outputs, targets)\n#         return loss\n\n# class VanillaRNN(RNN_LSTM_Base):\n#     def __init__(self, in_size, hid_size_rnn, hid_size_lin, out_size, n_layers=1):\n#         super(VanillaRNN, self).__init__()\n#         # Define dimensions for the layers\n#         self.input_size = in_size\n#         self.hidden_size_rnn = hid_size_rnn\n#         self.hidden_size_lin = hid_size_lin\n#         self.output_size = out_size\n#         self.n_layers = n_layers\n#         # Defining the RNN layer\n#         self.rnn = nn.RNN(in_size, hid_size_rnn, n_layers, batch_first=True)\n#         # Defining the linear layer\n#         self.linear = nn.Linear(hid_size_lin, out_size)\n\n#     def forward(self, x):\n#         # x must be of shape (batch_size, seq_len, input_size)\n#         xb = x.view(x.size(0), x.size(1), self.input_size).double()\n#         # Initialize the hidden layer's array of shape (n_layers*n_dirs, batch_size, hidden_size_rnn)\n#         h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size_rnn, requires_grad=True).double()\n#         # out is of shape (batch_size, seq_len, num_dirs*hidden_size_rnn)\n#         out, hn = self.rnn(xb, h0)\n#         # out needs to be reshaped into dimensions (batch_size, hidden_size_lin)\n#         out = out.reshape(x.size(0), self.hidden_size_lin)\n#         out = nn.functional.relu(out)\n#         # Finally we get out in the shape (batch_size, output_size)\n#         out = self.linear(out)\n#         return out\n\n\n#     def fit(epochs, lr, model, train_loader, test_loader, opt_func=torch.optim.SGD):\n#         optimizer = opt_func(model.parameters(), lr)\n#         for epoch in range(epochs):\n#             # Training phase\n#             model.train()\n#         for batch in train_loader:\n#             loss = model.training_step(batch)\n#             # Calculate gradients from chain rule\n#             loss.backward()\n#             # Apply gradient descent step\n#             optimizer.step()\n#             # Remove gradients for next iteration\n#             optimizer.zero_grad()\n#         return 'Trained for {} epochs'.format(epochs)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:42:48.334991Z","iopub.execute_input":"2023-06-10T00:42:48.335735Z","iopub.status.idle":"2023-06-10T00:42:48.342576Z","shell.execute_reply.started":"2023-06-10T00:42:48.335691Z","shell.execute_reply":"2023-06-10T00:42:48.341807Z"},"trusted":true},"execution_count":28,"outputs":[]}]}