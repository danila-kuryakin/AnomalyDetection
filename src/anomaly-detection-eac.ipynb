{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorchvideo \n!pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:31:46.048926Z","iopub.execute_input":"2023-05-22T15:31:46.049510Z","iopub.status.idle":"2023-05-22T15:32:11.520530Z","shell.execute_reply.started":"2023-05-22T15:31:46.049451Z","shell.execute_reply":"2023-05-22T15:32:11.519221Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorchvideo in /opt/conda/lib/python3.7/site-packages (0.1.5)\nRequirement already satisfied: iopath in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (0.1.10)\nRequirement already satisfied: parameterized in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (0.9.0)\nRequirement already satisfied: av in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (10.0.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (2.6.3)\nRequirement already satisfied: fvcore in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (0.1.5.post20221221)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (2.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (4.64.1)\nRequirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.1.8)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.21.6)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (9.4.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (4.4.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (2.7.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: mediapipe in /opt/conda/lib/python3.7/site-packages (0.9.0.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mediapipe) (3.5.3)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (from mediapipe) (4.5.4.60)\nRequirement already satisfied: protobuf<4,>=3.11 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (3.20.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mediapipe) (1.21.6)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (23.1.21)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (22.2.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (23.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (4.38.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (1.4.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (9.4.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport math\nimport cv2\nimport pickle\nimport random\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision import transforms\n\nimport mediapipe as mp\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-22T15:32:11.524305Z","iopub.execute_input":"2023-05-22T15:32:11.524637Z","iopub.status.idle":"2023-05-22T15:32:11.533891Z","shell.execute_reply.started":"2023-05-22T15:32:11.524602Z","shell.execute_reply":"2023-05-22T15:32:11.532806Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from pytorchvideo.data.encoded_video import EncodedVideo\nfrom pytorchvideo.data.encoded_video_pyav import EncodedVideoPyAV\nfrom torchvision.transforms import Compose, Lambda\n\nfrom torchvision.transforms._transforms_video import (\n    CenterCropVideo,\n    NormalizeVideo,\n)\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    ShortSideScale,\n    UniformTemporalSubsample,\n    UniformCropVideo\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.535441Z","iopub.execute_input":"2023-05-22T15:32:11.535775Z","iopub.status.idle":"2023-05-22T15:32:11.545160Z","shell.execute_reply.started":"2023-05-22T15:32:11.535746Z","shell.execute_reply":"2023-05-22T15:32:11.543553Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/mixkit'\nframe_width = 1920\nframe_height = 1080\n\nout = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc(*'XVID'), 30, (frame_width,frame_height))\n\nfor root, dirs, files in os.walk(path):\n    for video_path in files:\n        print(video_path)\n        cap = cv2.VideoCapture(os.path.join(root, video_path))\n        while (cap.isOpened()):\n            ret, frame = cap.read()\n            if ret == True:\n                out.write(cv2.resize(frame, (frame_width,frame_height)))\n            else:\n                break\n\n# out.release()\n            \n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.550584Z","iopub.execute_input":"2023-05-22T15:32:11.550876Z","iopub.status.idle":"2023-05-22T15:32:11.563006Z","shell.execute_reply.started":"2023-05-22T15:32:11.550848Z","shell.execute_reply":"2023-05-22T15:32:11.561931Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# path = '/kaggle/input/mixkit'\n# frame_width = 1920\n# frame_height = 1080\n\n# list_path = ['/kaggle/input/mixkit/surprise1.mp4',\n#             '/kaggle/input/mixkit/anger1.mp4',\n#             '/kaggle/input/mixkit/sadness2.mp4',\n#             '/kaggle/input/mixkit/happy1.mp4']\n\n# out = cv2.VideoWriter('outpy6.mp4',cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width,frame_height))\n\n# total = 0\n# print('video_path frames total_frames')\n# for video_path in list_path:\n#     counter = 0\n#     cap = cv2.VideoCapture(os.path.join(root, video_path))\n#     while (cap.isOpened()):\n#         ret, frame = cap.read()\n#         if ret == True:\n#             counter += 1\n#             total += 1\n#             out.write(cv2.resize(frame, (frame_width,frame_height)))\n#         else:\n#             break\n#     print(video_path, counter, total)\n\n# # out.release()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.564582Z","iopub.execute_input":"2023-05-22T15:32:11.565013Z","iopub.status.idle":"2023-05-22T15:32:11.573289Z","shell.execute_reply.started":"2023-05-22T15:32:11.564975Z","shell.execute_reply":"2023-05-22T15:32:11.572309Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def add_g(image_array, mean=0.0, var=30):\n    std = var ** 0.5\n    image_add = image_array + np.random.normal(mean, std, image_array.shape)\n    image_add = np.clip(image_add, 0, 255).astype(np.uint8)\n    return image_add\n\ndef flip_image(image_array):\n    return cv2.flip(image_array, 1)\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef generate_flip_grid(w, h, device):\n    # used to flip attention maps\n    x_ = torch.arange(w).view(1, -1).expand(h, -1)\n    y_ = torch.arange(h).view(-1, 1).expand(-1, w)\n    grid = torch.stack([x_, y_], dim=0).float().to(device)\n    grid = grid.unsqueeze(0).expand(1, -1, -1, -1)\n    grid[:, 0, :, :] = 2 * grid[:, 0, :, :] / (w - 1) - 1\n    grid[:, 1, :, :] = 2 * grid[:, 1, :, :] / (h - 1) - 1\n    grid[:, 0, :, :] = -grid[:, 0, :, :]\n    return grid","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.574937Z","iopub.execute_input":"2023-05-22T15:32:11.575442Z","iopub.status.idle":"2023-05-22T15:32:11.589025Z","shell.execute_reply.started":"2023-05-22T15:32:11.575396Z","shell.execute_reply":"2023-05-22T15:32:11.587683Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n    \n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=8631, include_top=True):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.include_top = include_top\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, \n                  return_indices=False, ceil_mode=False)\n        \n        \n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        \n        if not self.include_top:\n            return x\n        \n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.590805Z","iopub.execute_input":"2023-05-22T15:32:11.591414Z","iopub.status.idle":"2023-05-22T15:32:11.629467Z","shell.execute_reply.started":"2023-05-22T15:32:11.591376Z","shell.execute_reply":"2023-05-22T15:32:11.628432Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    \n    def __init__(self, pretrained=True, num_classes=7):\n        super(Model, self).__init__()\n        resnet50 = ResNet(Bottleneck, [3, 4, 6, 3])        \n        self.features = nn.Sequential(*list(resnet50.children())[:-2])  \n        self.features2 = nn.Sequential(*list(resnet50.children())[-2:-1])  \n        self.fc = nn.Linear(2048, 7)  \n        \n        \n    def forward(self, x):        \n        x = self.features(x)\n        #### 1, 2048, 7, 7\n        feature = self.features2(x)\n        #### 1, 2048, 1, 1\n        \n        feature = feature.view(feature.size(0), -1)\n        output = self.fc(feature)\n        \n        params = list(self.parameters())\n        fc_weights = params[-2].data\n        fc_weights = fc_weights.view(1, 7, 2048, 1, 1)\n        fc_weights = Variable(fc_weights, requires_grad = False)\n\n        # attention\n        feat = x.unsqueeze(1) # N * 1 * C * H * W\n        hm = feat * fc_weights\n        hm = hm.sum(2) # N * self.num_labels * H * W\n\n        return output, hm","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.633175Z","iopub.execute_input":"2023-05-22T15:32:11.633944Z","iopub.status.idle":"2023-05-22T15:32:11.646654Z","shell.execute_reply.started":"2023-05-22T15:32:11.633904Z","shell.execute_reply":"2023-05-22T15:32:11.645128Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"VAL_CSV = pd.read_csv(\n    '/kaggle/input/ckp-video/val.txt',\n    sep=' ',\n)\n\nVAL_DIR = '/kaggle/input/ckp-video/Val'\n\nTRAIN_CSV = pd.read_csv(\n    '/kaggle/input/ckp-video/train.txt',\n    sep=' ',\n)\n\nTRAIN_DIR = '/kaggle/input/ckp-video/Train'\n\nweight_path = '/kaggle/input/eac-raf-weight/epoch55_acc_0.897001.pth'\n\ndevice_name=0\nworkers=2\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.648584Z","iopub.execute_input":"2023-05-22T15:32:11.649042Z","iopub.status.idle":"2023-05-22T15:32:11.697735Z","shell.execute_reply.started":"2023-05-22T15:32:11.649002Z","shell.execute_reply":"2023-05-22T15:32:11.696696Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"label = {'name': ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger', 'Neutral'],\n         'color': ['blue', 'orange', 'green', 'red', 'darkviolet', 'brown', 'pink']}","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.703741Z","iopub.execute_input":"2023-05-22T15:32:11.704053Z","iopub.status.idle":"2023-05-22T15:32:11.709584Z","shell.execute_reply.started":"2023-05-22T15:32:11.704022Z","shell.execute_reply":"2023-05-22T15:32:11.708398Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def plot_probability(data, title,  figsize, save_dir):\n    file_name = os.path.join(save_dir, title + '.png')    \n    fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(data)\n    ax.set_title(title)\n    for index, line in enumerate(plt.gca().get_lines()):\n        line.set_color(label['color'][index])\n    \n    ax.legend(label['name'],\n                  loc='upper left')\n    \n    fig.savefig(file_name)\n    plt.close(fig)\n        \ndef plot_classes(data, title,  figsize):\n#     index = 0\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(data)\n    ax.set_title(title)\n    ax.set_ylim([-1, 7])\n    \ndef save_data(data, title, save_dir):\n    file_name = os.path.join(save_dir, title + '.txt')\n    with open(file_name, 'w') as open_file:\n        open_file.write('index probabilities\\n')\n        for index, row in enumerate(data):\n            open_file.write(str(index) + ' '+ ' '.join(str(r) for r in row) + '\\n')\n        open_file.close()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.711014Z","iopub.execute_input":"2023-05-22T15:32:11.711792Z","iopub.status.idle":"2023-05-22T15:32:11.725666Z","shell.execute_reply.started":"2023-05-22T15:32:11.711750Z","shell.execute_reply":"2023-05-22T15:32:11.724591Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def face_detection(image, mp_face_detection):\n    h, w, _ = image.shape\n    with mp_face_detection.FaceDetection(\n        model_selection=1, min_detection_confidence=0.5) as face_detection:\n\n        results = face_detection.process(\n            cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        detections = results.detections\n        if detections:\n            detection = detections[0]\n            x1 = int(w * detection.location_data.relative_bounding_box.xmin)\n            y1 = int(h * detection.location_data.relative_bounding_box.ymin)\n\n            if x1 < 0:\n                x1 = 0\n            if y1 < 0:\n                y1 = 0\n\n            width = w * detection.location_data.relative_bounding_box.width\n            height = h * detection.location_data.relative_bounding_box.height\n            x2 = int(w * detection.location_data.relative_bounding_box.xmin + width)\n            y2 = int(h * detection.location_data.relative_bounding_box.ymin + height)\n            if x2 > w:\n                x2 = w\n            if y2 > h:\n                y2 = h\n            return [x1, y1, x2, y2]\n    return None","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.726841Z","iopub.execute_input":"2023-05-22T15:32:11.727103Z","iopub.status.idle":"2023-05-22T15:32:11.741216Z","shell.execute_reply.started":"2023-05-22T15:32:11.727076Z","shell.execute_reply":"2023-05-22T15:32:11.740183Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def detect(model, video_path, mp_face_detection, save_dir):\n    probability = []\n    class_list = []\n\n    eval_transforms = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])])\n\n    cap = cv2.VideoCapture(video_path)\n    while (cap.isOpened()):\n        ret, frame = cap.read()\n        if ret == True:\n            coord = face_detection(frame, mp_face_detection)\n            if coord:\n                (x1, y1, x2, y2) = coord\n                face = frame[y1:y2, x1:x2, :]\n            else:\n                face = frame\n\n            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n            face = cv2.resize(face, (224, 224))\n#             plt.imshow(face)\n            face = eval_transforms(face)\n            face = face.reshape((1, 3, 224, 224))\n            face = face.to(device)\n\n            outputs = model(face)\n            sm = softmax(outputs[0])\n            am = torch.argmax(outputs[0])\n            probability.append(sm.detach().cpu().numpy()[0].tolist())\n            class_list.append(am.detach().cpu().tolist())\n        else:\n            break\n    probability = np.array(probability)\n    class_ar = np.array(class_list)\n    \n    video_name = video_path.split('/')[-1]\n    video_name = video_name.split('.')[0]\n    \n    plot_probability(probability, video_name, (8, 5), save_dir)\n    save_data(probability, video_name, save_dir)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.743141Z","iopub.execute_input":"2023-05-22T15:32:11.743978Z","iopub.status.idle":"2023-05-22T15:32:11.758817Z","shell.execute_reply.started":"2023-05-22T15:32:11.743939Z","shell.execute_reply":"2023-05-22T15:32:11.757793Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"mp_face_detection = mp.solutions.face_detection\n\nvideo_path = '/kaggle/input/ckp-video/Train/S503_001.avi'\n\nmodel = Model()\nsoftmax = nn.Softmax(dim=1)\nmodel.load_state_dict(torch.load(weight_path)['model_state_dict'])\ndevice = torch.device(device_name)\nmodel.to(device)\n\nsave_dir = 'val'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\nfor name in VAL_CSV['name']:\n    video_path = os.path.join(VAL_DIR, name)\n    detect(model, video_path, mp_face_detection, save_dir)\n    \n    \nsave_dir = 'train'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\nfor name in TRAIN_CSV['name']:\n    video_path = os.path.join(TRAIN_DIR, name)\n    detect(model, video_path, mp_face_detection, save_dir)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:32:11.761290Z","iopub.execute_input":"2023-05-22T15:32:11.762078Z","iopub.status.idle":"2023-05-22T15:38:27.320916Z","shell.execute_reply.started":"2023-05-22T15:32:11.762038Z","shell.execute_reply":"2023-05-22T15:38:27.319817Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('/kaggle/working/train', 'zip', '/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:38:27.322755Z","iopub.execute_input":"2023-05-22T15:38:27.323128Z","iopub.status.idle":"2023-05-22T15:38:27.864957Z","shell.execute_reply.started":"2023-05-22T15:38:27.323081Z","shell.execute_reply":"2023-05-22T15:38:27.863974Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/train.zip'"},"metadata":{}}]}]}